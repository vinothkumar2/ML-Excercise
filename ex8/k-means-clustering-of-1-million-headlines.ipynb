{"cells":[{"metadata":{"_uuid":"d56b7961b91d7f67554ec48bd1d2485fb7419120","_cell_guid":"8b9d6583-6ae1-4bb6-970c-c80d42dfebdb"},"cell_type":"markdown","source":"# About the dataset\nThis contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.)\nSite: http://www.abc.net.au/\nPrepared by Rohit Kulkarni\n"},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false,"collapsed":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/abcnews-date-text.csv\",error_bad_lines=False,usecols =[\"headline_text\"])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a498ee778ab763e0801b8f9cf14e1d4d01f38846","_cell_guid":"3e44d856-a323-45ac-b7cc-80d77385060f","trusted":false,"collapsed":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb590852f097f66ea53be9a970789430fc3f6a63","_cell_guid":"ff887c6d-0470-4f62-860b-9457b223bb8c"},"cell_type":"markdown","source":"# Deleting dupliate headlines(if any)"},{"metadata":{"_uuid":"1e0143660cbb59acf14ed07c847fd9bc3ca85045","_cell_guid":"42392880-315c-41bf-98e8-a1cbfab72f6e","trusted":false,"collapsed":true},"cell_type":"code","source":"data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4e5e82d7c6fb0e7b14f8b5772bea14e448f88fcc","_cell_guid":"9f5ff611-397e-45b7-9616-7bc33f6e81bb","trusted":false},"cell_type":"code","source":"data = data.drop_duplicates('headline_text')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f022fdf6441499ed52b34c063240f4f28b2ff3a5","_cell_guid":"f1bb8d35-27aa-4ff4-9a39-4329517aa6a4"},"cell_type":"markdown","source":"# NLP "},{"metadata":{"_uuid":"d2f1e1b88fb7b29fd47c249be3af044ef1e2a246","_cell_guid":"7ecba7c7-84d0-426b-aa9c-9ca05d45da75"},"cell_type":"markdown","source":"# Preparing data for vectorizaion\nHowever, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”.\n\nIn terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words.\n\n# TF-IDF\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n\nVariations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n\nOne of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."},{"metadata":{"collapsed":true,"_uuid":"9c1c23ecabae8217a9aa8f90371f2a30053cc6f1","_cell_guid":"c7e595ab-440c-4ad7-98e4-4358cc724d8c","trusted":false},"cell_type":"code","source":"punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\nstop_words = text.ENGLISH_STOP_WORDS.union(punc)\ndesc = data['headline_text'].values\nvectorizer = TfidfVectorizer(stop_words = stop_words)\nX = vectorizer.fit_transform(desc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10af64e15e2f08c30da71b847432eedc2aece199","_cell_guid":"eb56971e-5412-4138-a4cd-0e14844796be","trusted":false,"collapsed":true},"cell_type":"code","source":"word_features = vectorizer.get_feature_names()\nprint(len(word_features))\nprint(word_features[5000:5100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e480f5b88938660f05c09f75af5f9f58d7110096","_cell_guid":"871b1bd6-c411-4ff6-a784-9b376a0db4e6"},"cell_type":"markdown","source":"# Stemming\nStemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word.\n\n# Tokenizing\nTokenization is breaking the sentence into words and punctuation,"},{"metadata":{"collapsed":true,"_uuid":"5d25104db183624b990a1d64e10cc618fd8ee715","_cell_guid":"536a1a88-48a3-43d0-b368-ccf31947e5b1","trusted":false},"cell_type":"code","source":"stemmer = SnowballStemmer('english')\ntokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n\ndef tokenize(text):\n    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d190c4d6b9cb52ad70a021e3475704538b47f85f","_cell_guid":"9877cf33-ebe8-46e4-b26e-c6f673617517"},"cell_type":"markdown","source":"# Vectorization with stop words(words irrelevant to the model), stemming and tokenizing"},{"metadata":{"_uuid":"5fcc93fa3093f30d181ffa38b4ca46b133316952","scrolled":false,"_cell_guid":"18e1d30c-5515-4c0d-89e4-6af99658bee3","trusted":false,"collapsed":true},"cell_type":"code","source":"vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\nX2 = vectorizer2.fit_transform(desc)\nword_features2 = vectorizer2.get_feature_names()\nprint(len(word_features2))\nprint(word_features2[:50]) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"244a3015e5b4f4f84c174586fa875f5cf49cff1d","_cell_guid":"acdc11f4-7b5c-4aee-8c42-1a752bffbc2e","trusted":false},"cell_type":"code","source":"vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\nX3 = vectorizer3.fit_transform(desc)\nwords = vectorizer3.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5f2e66c25d17527b78ae1a1fe174f1ac6310286","_cell_guid":"e8a8c1ed-8970-49e0-bae5-0e0d85abea84"},"cell_type":"markdown","source":"For this, we will use k-means clustering algorithm.\n# K-means clustering\n(Source [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm) )\n![http://gdurl.com/5BbP](http://gdurl.com/5BbP)\""},{"metadata":{"_uuid":"39ba3fa0f53454111495da2f7e7719572afec93a","_cell_guid":"c9a1312e-45f7-44e4-9407-13012bdf97ce"},"cell_type":"markdown","source":"# Elbow method to select number of clusters\nThis method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified. Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.\n# Basically, number of clusters = the x-axis value of the point that is the corner of the \"elbow\"(the plot looks often looks like an elbow)"},{"metadata":{"collapsed":true,"_uuid":"992bea80b2647c4f4e564bb020ce8eab07db6b78","scrolled":true,"_cell_guid":"c72b26ab-4bef-44e6-b854-5bca3cd1f217","trusted":false},"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n    kmeans.fit(X3)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.savefig('elbow.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5184cf4e47df719970d89c5ea8e15d4b9eaa1e5","_cell_guid":"2dd63611-35d6-4c6e-b076-c10e18a3b10c"},"cell_type":"markdown","source":"As more than one elbows have been generated, I will have to select right amount of clusters by trial and error. So, I will showcase the results of different amount of clusters to find out the right amount of clusters."},{"metadata":{"collapsed":true,"_uuid":"0ed982322b3a0fecb997e88ef0fb2f681c5a801c","scrolled":false,"_cell_guid":"e096b262-a06c-4f0a-9c50-2ef4bda9b926","trusted":false},"cell_type":"code","source":"print(words[250:300])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f463135cb544f6ffa999861c7bf049cbbede8fe6","_cell_guid":"0ca23daa-681a-4cb8-93ae-d2d2ac137604"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"6992f369b10d54adf27ecdcf24c6f57deabf466f","_cell_guid":"a3c414e5-70b2-4c63-a46a-f9c82e406e4b"},"cell_type":"markdown","source":"# 3 Clusters"},{"metadata":{"collapsed":true,"_uuid":"e0e187a022ec9032385f715d87cfbb865a11698d","scrolled":false,"_cell_guid":"b395dddc-8acb-40a8-825e-a6f3a615809e","trusted":false},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)\nkmeans.fit(X3)\n# We look at 3 the clusters generated by k-means.\ncommon_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\nfor num, centroid in enumerate(common_words):\n    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4750051c6cbdc6d262f31a94f5c4776eb020644a","_cell_guid":"05f80701-83af-49c1-a7b8-df8c20396bb9"},"cell_type":"markdown","source":"# 5 Clusters"},{"metadata":{"collapsed":true,"_uuid":"2813cee7e61b966b179b806d29b222c55551ee35","scrolled":false,"_cell_guid":"137b7e2c-970f-4f2f-929e-c7b2bd2a8004","trusted":false},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1)\nkmeans.fit(X3)\n# We look at 5 the clusters generated by k-means.\ncommon_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\nfor num, centroid in enumerate(common_words):\n    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4faeeea430f9f679a594b2c7f1ef1c857611ade5","_cell_guid":"e5ab0cdc-1178-486d-a982-3457aa69d234"},"cell_type":"markdown","source":"# 6 Clusters"},{"metadata":{"collapsed":true,"_uuid":"73c1c5bfafe12fd9bd57ab0c3bd07f60630f05a8","_cell_guid":"de4d9fbb-2462-4491-899f-b1e28e1a7697","trusted":false},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 6, n_init = 20, n_jobs = 1)\nkmeans.fit(X3)\n# We look at 6 the clusters generated by k-means.\ncommon_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\nfor num, centroid in enumerate(common_words):\n    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7225da580975f8589615a8fd5232ba3c80780845","_cell_guid":"bff92434-ec9d-4e6b-a4a2-72ec4ea6f3bc"},"cell_type":"markdown","source":"# 8 Clusters"},{"metadata":{"collapsed":true,"_uuid":"cf3c5af56aa05f6679effe85774207fe824255a1","_cell_guid":"4134a6d2-09aa-4cf5-9821-d95d0482c4ac","trusted":false},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)\nkmeans.fit(X3)\n# Finally, we look at 8 the clusters generated by k-means.\ncommon_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\nfor num, centroid in enumerate(common_words):\n    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4717568e34623b3b88f033670c20f01f7ddd63a6","_cell_guid":"a168498b-1322-4010-b462-596bc0ba186b"},"cell_type":"markdown","source":"Because even I didn't know what kind of clusters would be generated, I will describe them in comments."}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}