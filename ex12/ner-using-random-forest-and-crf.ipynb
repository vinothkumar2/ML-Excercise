{"cells":[{"metadata":{"_uuid":"9597155d391999e0983ebf0fb0c3632373fce71f"},"cell_type":"markdown","source":"# Named Entity Recognition using GMB(Groningen Meaning Bank) corpus"},{"metadata":{"_uuid":"d2d04420a76e056c6b072975f294b4eb35408073"},"cell_type":"markdown","source":"## Objective\nNamed Entity Recognition for annotated corpus using GMB(Groningen Meaning Bank) corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set."},{"metadata":{"_uuid":"157d1ec2211207c91fb4ef948dc6bd56474f79cf"},"cell_type":"markdown","source":"## Background"},{"metadata":{"_uuid":"340656cf592652d395e87089642473006eb75c1b"},"cell_type":"markdown","source":"### Understanding Named Entity Recognition\nNamed Entity Recognition or Named Entity Recognition and Classification (NERC) is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories. Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on.\n<br><br>\n**Commonly Used Types of Named Entity:**"},{"metadata":{"_uuid":"8a38fc7a6904dff961460e34c765a16d7d437dca"},"cell_type":"markdown","source":"| NE Type      | Examples                                |\n|--------------|-----------------------------------------|\n| ORGANIZATION | Georgia-Pacific Corp., WHO              |\n| PERSON       | Eddy Bonte, President Obama             |\n| LOCATION     | Murray River, Mount Everest             |\n| DATE         | June, 2008-06-29                        |\n| TIME         | two fifty a m, 1:30 p.m.                |\n| MONEY        | 175 million Canadian Dollars, GBP 10.40 |\n| PERCENT      | twenty pct, 18.75 %                     |\n| FACILITY     | Washington Monument, Stonehenge         |\n| GPE          | South East Asia, Midlothian             |"},{"metadata":{"_uuid":"8db9664944dbe2613ec5ad9bb700d6b0f4527923"},"cell_type":"markdown","source":"The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type. \n<br><br>\nNamed entity recognition is a task that is well-suited to the type of classifier-based approach. In particular, a tagger can be built that labels each word in a sentence using the IOB format, where chunks are labelled by their appropriate type.\n<br><br>\nThe IOB Tagging system contains tags of the form:<br>\n**B** - {CHUNK_TYPE} – for the word in the Beginning chunk<br>\n**I** - {CHUNK_TYPE} – for words Inside the chunk<br>\n**O** – Outside any chunk<br>\n<br>\nThe IOB tags are further classified into the following classes – \n<br>\n**geo** = Geographical Entity<br>\n**org** = Organization<br>\n**per** = Person<br>\n**gpe** = Geopolitical Entity<br>\n**tim** = Time indicator<br>\n**art** = Artifact<br>\n**eve** = Event<br>\n**nat** = Natural Phenomenon<br>"},{"metadata":{"_uuid":"04bdc9c2766d38b9e278bfab3de22dc94fbc5d7e"},"cell_type":"markdown","source":"## Data source\n\nThe dataset an extract from GMB corpus which is tagged, annotated and built specifically to train the classifier to predict named entities such as name, location, etc. GMB is a fairly large corpus with a lot of annotations. Unfortunately, GMB is not perfect. It is not a gold standard corpus, meaning that it’s not completely human annotated and it’s not considered 100% correct. The corpus is created by using already existed annotators and then corrected by humans where needed."},{"metadata":{"_uuid":"9263d3a4abb7a474ccfaee2706547f7e5394a402"},"cell_type":"markdown","source":"## Analysis pipeline - the OSEMN approach\n\n - **Obtain the data** <br>\n The dataset is an extract of the GMB corpus which is tagged, annotated. It is in tab separated text file.\n - **Scrubbing / Cleaning the data** <br>\n Initial data exploration and preparation for analysis\n - **Exploring / Visualizing our data** <br>\n Basic EDA to understand the data\n - **Modeling the data** <br>\n Create classification models for NER\n     -  Simple tree based model - RandomForest\n     -  State of the art CRF model\n     -  Hyperparameter tuning\n - **Interpreting the results** <br>\n Analysing the performance and further steps to improve"},{"metadata":{"_uuid":"953a25dd99473b21f84af01fe40e8e3b3f2baafb"},"cell_type":"markdown","source":"### Environment set-up and loading dependencies\n\nAnaconda is used to do the analysis, which is an easy-to-install, free, enterprise-ready Python distribution for data analytics, processing, and scientific computing.\nDependencies used are below -"},{"metadata":{"trusted":true,"_uuid":"8829869324ff6ea1aa98d6c8de2e944ced88f64e"},"cell_type":"code","source":"#Data analysis\nimport pandas as pd\nimport numpy as np\n#Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\nsns.set(font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n#Modeling\nfrom sklearn.cross_validation import cross_val_predict, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn_crfsuite import CRF, scorers, metrics\nfrom sklearn_crfsuite.metrics import flat_classification_report\nfrom sklearn.metrics import classification_report, make_scorer\nfrom sklearn.grid_search import RandomizedSearchCV\nimport scipy.stats\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bb701b4b4332521d2c67cdcbd087298125274f9"},"cell_type":"markdown","source":"## Obtain the data"},{"metadata":{"trusted":true,"_uuid":"0d67521bac1dbafad1cc4d622ad15865d6b42f55"},"cell_type":"code","source":"data = pd.read_csv(\"../input/GMB_dataset.txt\", sep=\"\\t\", header=None, encoding=\"latin1\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f719a49ad3d05e850c4738155ecb3b6f3e155e0"},"cell_type":"markdown","source":"## Scrubbing / Cleaning the data "},{"metadata":{"trusted":false,"_uuid":"ecdab848648502e9feb9f6c8c323d2b022dc1a21"},"cell_type":"code","source":"#Let us take a sneak-peak into the dataset first\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"52930499bfb7944ec8799592479869732774d882"},"cell_type":"code","source":"#The dataset does not have any header currently. We can use the first row as a header as it has the relevant headings.\n#We will make the first row as the heading, remove the first row and re-index the dataset\ndata.columns = data.iloc[0]\ndata = data[1:]\ndata.columns = ['Index','Sentence #','Word','POS','Tag']\ndata = data.reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9d681b3cf00288c589505e7be694e9a93185c6fb"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4a9460ed0745e5f90b4af64715a82ba50623d5c8"},"cell_type":"code","source":"#We have 66161 samples and 5 features. We will understand them in detail in the exploration step.\n#Lets check for any missing values in the dataset\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb5f9dd5f0ad3e579c1ca1f366dbbd8328320b48"},"cell_type":"markdown","source":"*Seems the dataset does not contain any missing values, and we are good to proceed to the exploration step where we will try to understand the data more.*"},{"metadata":{"_uuid":"782355d1cc76dd3404217f26777c76abae2ea774"},"cell_type":"markdown","source":"## Exploring / Visualizing our data"},{"metadata":{"_uuid":"3075f338d243e96a6d6cb033c760bbcd3b987d4c"},"cell_type":"markdown","source":"*Before going further, we will try to understand what the dataset is all about and what all the features mean. This is important in order to understand how the classifiers will perform and help us interpret the results.*"},{"metadata":{"trusted":false,"_uuid":"9b539c8e4298cdc77ca7ee314e0d0f6240d09a51"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"849665d5eacb92cdec68f1da5eaed2e2c776ad65"},"cell_type":"markdown","source":"The dataset has the following columns or features - <br>\n - **Index** - Index numbers for each word [Numeric type]\n - **Sentence #** - The number of sentences in the dataset (We will find the number of sentences below) [Numeric type]\n - **Word** - The words in the sentence [Character type]\n - **POS** - Parts Of Speech tags, these are tags given to the type of words as per the Penn TreeBank Tagset [Categorical type]\n - **Tag** - The tags given to each word based on the IOB tagging system described above (Target variable) [Categorical type]"},{"metadata":{"_uuid":"439cbfa74744bc85cf26123fe57857fa51ad1ad8"},"cell_type":"markdown","source":"*Since the dataset is annotated with POS and Tags, we will build a simple class to combine the words into a sentence. This will help us understand the extract of the dataset.*"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3df00ab579a962a2d668b33144205d311a3212b2"},"cell_type":"code","source":"# A class to retrieve the sentences from the dataset\nclass getsentence(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1.0\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5587d76f53c51fcc18663f2c7e28b47a522a0e92"},"cell_type":"code","source":"getter = getsentence(data)\nsentences = getter.sentences\n#This is how a sentence will look like. \nprint(sentences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"09913bb9b918cc7dbabe26a7a5f018e78fb99c11"},"cell_type":"code","source":"#Lets find the number of words in the dataset\nwords = list(set(data[\"Word\"].values))\nn_words = len(words)\nprint(n_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31350a16079b9a440e68df354352c0438bb6cf2f"},"cell_type":"code","source":"#Lets visualize how the sentences are distributed by their length\nplt.style.use(\"ggplot\")\nplt.hist([len(s) for s in sentences], bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7a99de8624d0ed76fe88593d580cf91870df2c5"},"cell_type":"markdown","source":"*It seems most of the sentences are 20-30 words long, and the distribution is normal.*"},{"metadata":{"trusted":false,"_uuid":"ee51c6fc92eaca6982b732f56b10df914d5a05e1"},"cell_type":"code","source":"#Lets find out the longest sentence length in the dataset\nmaxlen = max([len(s) for s in sentences])\nprint ('Maximum sentence length:', maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a16162425fd7da08022961d451f3c8db1b711a7"},"cell_type":"markdown","source":"*Now that we know the words and sentences, lets try to understand what sort of words each tag contains. This will help us in understanding what each tag type and sub-type represents.*"},{"metadata":{"trusted":false,"_uuid":"b36048e8cef556b3f154baf4e569f877ef9ce521"},"cell_type":"code","source":"#Words tagged as B-org\ndata.loc[data['Tag'] == 'B-org', 'Word'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b604b7d30791b6aebcc2041b7c4225994426099"},"cell_type":"code","source":"#Words tagged as I-org\ndata.loc[data['Tag'] == 'I-org', 'Word'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"92b73c60c24edaff592a406147636305fc059dcf"},"cell_type":"code","source":"#Words tagged as B-per\ndata.loc[data['Tag'] == 'B-per', 'Word'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bb2c564081e6d2ed47892cfe6dd7ff7c2d54a1a3"},"cell_type":"code","source":"#Words tagged as I-per\ndata.loc[data['Tag'] == 'I-per', 'Word'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"85a63e2b313148ec10589551e52f156120df2c64"},"cell_type":"code","source":"#Words tagged as B-geo\ndata.loc[data['Tag'] == 'B-geo', 'Word'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b57e2cced8a31ce7e9fd236caf06e5061bc498a"},"cell_type":"code","source":"#Words tagged as I-geo\ndata.loc[data['Tag'] == 'I-geo', 'Word'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"66bfa3473fd15864247e1c30f05e08fac76a20e2"},"cell_type":"code","source":"#Words distribution across Tags\nplt.figure(figsize=(15, 5))\nax = sns.countplot('Tag', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93e34c1e422031a41e53fe31096c8732dfbe7120"},"cell_type":"markdown","source":"*Quite surprising, most of the words are tagged as outside of any chunk. These words can be considered as fillers and their presence might impact the classifier performance as well. Lets check the dataset again without the O tags.*"},{"metadata":{"trusted":false,"_uuid":"a906aa05066eba0394b5a6b2aaf6173528e7a2f5"},"cell_type":"code","source":"#Words distribution across Tags without O tag\nplt.figure(figsize=(15, 5))\nax = sns.countplot('Tag', data=data.loc[data['Tag'] != 'O'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e28b1d59586da44e81eba2ea309d822c2223097"},"cell_type":"markdown","source":"*So our dataset mostly contains words related to geographical locations, geopolitical entities and person names.*"},{"metadata":{"trusted":false,"_uuid":"9f7fc30b26d854dc4a1d7ad820677aecdc4467ce"},"cell_type":"code","source":"#Words distribution across POS\nplt.figure(figsize=(15, 5))\nax = sns.countplot('POS', data=data, orient='h')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ec82341c9c724009fb0ee00d46fa678da15f5c9"},"cell_type":"markdown","source":"## Modeling the data\nWith the basic EDA done and understanding the dataset, we can move to the modeling stage. <br>\nSince the problem statement is a simple classification problem, we will start with a simple tree based model, Random Forest using a simple feature map. <br>\nSimple tree based models have been proven to provide decent performance in building NERC systems. Random Forest being one of the most popular tree based models can learn the underlying rules according to which terms are tagged. It is important that the classifier has proper features fed in to improve the performance. \n<br> <br>\nWe chose Random Forest because of the following reasons - <br>\nFirstly, RF is able to automatically construct correlation paths from the feature space, i.e. decision rules that correspond to the translation rules that we intend to capture. <br>\nSecondly, RF is considered one of the most accurate classifier available."},{"metadata":{"_uuid":"72df34844198d1e1a29f902208071e1c7c8ade0d"},"cell_type":"markdown","source":"### Performance metrics\n\nBefore we move to the modeling part, it is important to understand the performance metrics on the basis of which the models will be evaluated. Since we are dealing with Information Extraction, we will use the following metrics to evaluate the models - <br>\n - **Precision**\n - **Recall**\n - **F1 score**\n<br> <br>\nThe metrics mentioned above are calculated using True/False positives and True/False negatives respectively. <br>\n - **True Positives (TP)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n - **True Negatives (TN)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n - **False Positives (FP)** – When actual class is no and predicted class is yes. \n - **False Negatives (FN)** – When actual class is yes but predicted class in no. \n - **Precision** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. <br> <br>\n $ Precision = TP/TP+FP $\n <br> <br>\n - **Recall (Sensitivity)** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. <br><br>\n $ Recall = TP/TP+FN $\n <br><br>\n - **F1 score** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is the harmonic mean of the both Precision and Recall <br><br>\n $ \\begin{equation*}\n F1-Score = 2*\\frac{(Recall * Precision)}{(Recall + Precision)}\n \\end{equation*}\n $\n <br><br>\nFor a decent classifier, we would prefer high precision and recall values. Classification reports are used to obtain the values of these metrics in a text format per class. It is essential that the model is evaluated by these metrics per class to make sure we have a good model.\n"},{"metadata":{"_uuid":"afa3fe0d0551f96a0664c6bafb445fd4b150c7e3"},"cell_type":"markdown","source":"*Now we shall start with the modeling part where we create new features, create the model and evaluate it on the metrics stated above.*"},{"metadata":{"_uuid":"7bfcc811c64b28a16c6faec7d78d222f51575055"},"cell_type":"markdown","source":"### Random Forest classifier"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cece45c812c41dd96c2ce4d43359854a8c34911a"},"cell_type":"code","source":"#Simple feature map to feed arrays into the classifier. \ndef feature_map(word):\n    return np.array([word.istitle(), word.islower(), word.isupper(), len(word),\n                     word.isdigit(),  word.isalpha()])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d6e333c7dd0eacb78c4b1ed84aafeb4c62b2256c"},"cell_type":"code","source":"#We divide the dataset into train and test sets\nwords = [feature_map(w) for w in data[\"Word\"].values.tolist()]\ntags = data[\"Tag\"].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"e9b0eb6ed158f88dbea356fcbb1b98b120c3d9de"},"cell_type":"code","source":"#Lets see how the input array looks like\nprint(words[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cce10d60a3536f5b0fe1a9c6d4159296e70aac1"},"cell_type":"markdown","source":"*We will use 5 fold cross validation as an input parameter to the classifier, i.e. we will divide the dataset into 5 subsets and train-test on them. Some models like decision trees and neural networks are often be able to get 100% accuracy on the training data, but perform much worse on new data. Therefore, we will train on one subset and test on the other, and repeat for every subset so that the classifier classifies correctly on average and the performance estimate is not overly optimistic.*"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9e2c87822c80ce53295f6689683b1cd00d8e2078"},"cell_type":"code","source":"#Random Forest classifier\npred = cross_val_predict(RandomForestClassifier(n_estimators=20),X=words, y=tags, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"840a2edeac12bd86092d53cb5fd725e8524e1f30"},"cell_type":"code","source":"#Lets check the performance \nfrom sklearn.metrics import classification_report\nreport = classification_report(y_pred=pred, y_true=tags)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5ebcab7fee2314690a43bd7e24bc0fad3c2a411"},"cell_type":"markdown","source":"*Although we have a good average score, the model performed quite badly. The precision and recall values of most of the classes were 0. It seems the features which require the model to take proper decisions are missing. The model is basically memorizing words and tags, which will not suffice. The context information behind each word needs to be fed to the model as well so that the predictions are more accurate. <br>\nWe can either work on this model alone by improving the features or ensembling it with a more contextual model, or use a different model altogether.*"},{"metadata":{"_uuid":"da04f01d3d0b9b2b229f707abbbf1557e38e30c6"},"cell_type":"markdown","source":"### Conditional Random Fields classifier"},{"metadata":{"_uuid":"8468413eea273fab9c75021dda308b77f3b806d4"},"cell_type":"markdown","source":"A Conditional Random Field (CRF) is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. It is a supervised learning method which has been proven to be better than the tree based models when it comes to NER. Whereas a discrete classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in natural language processing) predicts sequences of labels for sequences of input samples. <br>\nWe will be using the LGBFS algorithm (Gradient descent using the L-BFGS method) and it works best using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning. Gradient Descent will be used as an optimization function."},{"metadata":{"_uuid":"43d0c8d0b95899d568e991f6bac03188ee383737"},"cell_type":"markdown","source":"*In order to use CRF, we will enhance the feature set and create more features which can be used by the model to predict the tags correctly. Since we need to take into account the context as well, we create features which will provide consecutive POS tags for each word. Also, we add new features such as upper, lower, digit, title etc. for each word and also consider the consecutive words in the list. In short, we try to provide a sequence of features to the model for each word - the sequence containing POS tags, capitalisations, type of word(title) etc.*"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5d03a92c58d658a4c380ae51dfad6b1f47e33773"},"cell_type":"code","source":"# Feature set\ndef word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n\n    features = {\n        'bias': 1.0,\n        'word.lower()': word.lower(),\n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit(),\n        'postag': postag,\n        'postag[:2]': postag[:2],\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper(),\n            '-1:postag': postag1,\n            '-1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['BOS'] = True\n\n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper(),\n            '+1:postag': postag1,\n            '+1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['EOS'] = True\n\n    return features","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"df98d881c4a4986422240134bed62c823f09abea"},"cell_type":"code","source":"def sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8ee5899c754b9a00e906b4e8019eac3f3d440a34"},"cell_type":"code","source":"#Creating the train and test set\nX = [sent2features(s) for s in sentences]\ny = [sent2labels(s) for s in sentences]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c05c6ea3a9f4749155dd5b9e0c85d2a6bfa100ef"},"cell_type":"code","source":"#Creating the CRF model\ncrf = CRF(algorithm='lbfgs',\n          c1=0.1,\n          c2=0.1,\n          max_iterations=100,\n          all_possible_transitions=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"53dac8074083755feefb60d55c269b89fa8057a4"},"cell_type":"code","source":"#We predcit using the same 5 fold cross validation\npred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9e412df7c7329d5be0cf4aeb3e64cf3ae8a3d27d"},"cell_type":"code","source":"#Lets evaluate the mode\nreport = flat_classification_report(y_pred=pred, y_true=y)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc54f98348a2d4126ed1c65baaab70c1b9f9bd91"},"cell_type":"markdown","source":"*Compared to the Random Forest classifier, the CRF classifier did better as the scores have improved. However, the precision and recall metrics of the classes individually have not improved. Maybe the model is again remembering words and not taking into the context information completely. We will try tuning the model manually to see if we can improve it.*"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b9d29d6d0bd078b49a12ab43a004b283ec6c03e0"},"cell_type":"code","source":"#Tuning the parameters manually, setting c1 = 10\ncrf2 = CRF(algorithm='lbfgs',\n          c1=10,\n          c2=0.1,\n          max_iterations=100,\n          all_possible_transitions=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"052d47eca5fdbe100d512905bfaa665f79e60fc3"},"cell_type":"code","source":"pred = cross_val_predict(estimator=crf2, X=X, y=y, cv=5)\nreport = flat_classification_report(y_pred=pred, y_true=y)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d235308ff8156ae1490c1ce3bf9f721b658c4a7"},"cell_type":"code","source":"crf2.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68bc0c9a3857d7b9c9444c324750ec5ca1d67eca"},"cell_type":"markdown","source":"*The average score has dropped but the individual precision and recall scores have improved. The model is trying to understand the context as well and not just remember words. We can see that there is a scope of improving the model. Maybe we can do it computationally and get a better model.*"},{"metadata":{"_uuid":"a398436f2d4474502efb1c20fc6a30cde84b602c"},"cell_type":"markdown","source":"### Hyperparameter tuning using Randomized CV Search"},{"metadata":{"trusted":false,"_uuid":"db9837c76e0ecdcaf5822fce63c45bbfbe0426fb"},"cell_type":"code","source":"#First we select all the tags that are relevant for us i.e. remove the 'O' tag from the list. \nlabels = list(crf2.classes_)\nlabels = list(filter(lambda a: a != 'O', labels))\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"97f53d4348c67b9897907fdf3aed3cff8486f627"},"cell_type":"code","source":"%%time\n#Now we will create the Randomized CV search model wherein we will use a modified F1 scorer model considering only the relevant labels\n# define fixed parameters and parameters to search\ncrf3 = CRF(\n    algorithm='lbfgs',\n    max_iterations=100,\n    all_possible_transitions=True\n)\nparams_space = {\n    'c1': scipy.stats.expon(scale=0.5),\n    'c2': scipy.stats.expon(scale=0.05),\n}\n\n# use the same metric for evaluation\nf1_scorer = make_scorer(metrics.flat_f1_score,\n                        average='weighted', labels=labels)\n\n# search\nrs = RandomizedSearchCV(crf, params_space,\n                        cv=3,\n                        verbose=1,\n                        n_jobs=-1,\n                        n_iter=50,\n                        scoring=f1_scorer)\nrs.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"49be7f8fc006048994c8b613e655c8b8d9d5f078"},"cell_type":"code","source":"#Lets check the best estimated parameters and CV score\nprint('Best parameters:', rs.best_params_)\nprint('Best CV score:', rs.best_score_)\nprint('Model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a65c067383673e62dd7f60b9405b4f9cdf51ee65"},"cell_type":"code","source":"#We sort the tags a bit so that they appear in an orderly fashion in the classification report\nsorted_labels = sorted(\n    labels,\n    key=lambda name: (name[1:], name[0]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"0f9cacae03a7f243c36c82f9f377343154213722"},"cell_type":"code","source":"#Now we create the model again using the best estimators\ncrf3 = rs.best_estimator_\ny_pred = crf3.predict(X)\nprint(metrics.flat_classification_report(\n    y, y_pred, labels=sorted_labels, digits=3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dad4bd25ef7f071c0857a2de6670e502a5f80aa"},"cell_type":"markdown","source":"*Now we have a good model with decent precision and recall scores for each class. We can fit this model and use it to predict tags given a sentence.*"},{"metadata":{"trusted":false,"_uuid":"def5941673d83aea677d94879bcd985d8f9b9a37"},"cell_type":"code","source":"crf3.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd83d363812b4ba6ef0f052f922e9d96ea0e3286"},"cell_type":"markdown","source":"*Now since we have good model, we can take sneak peek into the model and see how it is classifying and what all weights are assigned. ELI5 report has built-in support for several ML frameworks and provides a way to explain black-box models.*"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"a9d60478fc81bb9b73a349f3173fe0c97c4b7685"},"cell_type":"code","source":"eli5.show_weights(crf3, top=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c3113a548ac644d263ad64cbfd57116e42d48a"},"cell_type":"markdown","source":"## Interpreting the results"},{"metadata":{"_uuid":"1f5caa8fa2cfd42cd97e7526ef6e66f1dd6dbb65"},"cell_type":"markdown","source":"*Let's try to understand the report in brief. <br>\nCRFsuite CRF models use two kinds of features: state features and transition features. <br>\nTransition features make sense: at least model learned that **I-ENITITY** must follow **B-ENTITY**. It also learned that some transitions are unlikely, e.g. it is not common in this dataset to have a location right after an organization name, i.e. **I-ORG** -> **B-LOC** has a negative weight.\nWe can expect that **O** -> **I-ENTITY** transitions to have large negative weights because they are impossible. <br>\nWe can also see that **B-ORG** -> **I-ORG** has positive weights which makes sense, as the First names are always followed by the inner names, the same applies for organisations.*"},{"metadata":{"_uuid":"e8cdf14c1c24c4da1112014e06f2b99b156a83a6"},"cell_type":"markdown","source":"Now we have a good model fit using the CRF classifier. The model is not perfect and it can be further improved by various ways -<br>\n - Adding more features to the model e.g. combination of words with gives a proper meaning - e.g. President Obama, Park Street etc.\n - Using a larger corpus from the GMB dataset.\n - Current implementation considers only 2 hyper parameters in CV search, however, the CRF model offers more parameters which can be further tuned to improve on the performance. \n - Using a LSTM neural network and creating a ensemble model with CRF. Since NER usually deals with a huge corpus, neural networks are very efficient in identifying patterns in the data provide a better model.\n - Using state of the art deep learning models using Keras or Tensor Flow - deep learning models are known to outperform every other model when it comes to huge datasets, therefore they can be used to create a superior NER classifier.\n - Using different algorithms in the CRF model. Currently it uses Gradient Descent but Stochastic Gradient Descent can be used to create a faster model. \n<br> <br>\nIt is possible to improve the model performance from a current 95% F1-score to about 98% using the above techniques. It requires huge computing resources to achieve this performance boost, unfortunately. \n"},{"metadata":{"_uuid":"af538ee921a8ae79c0e1eca14d1f5f09280ad706"},"cell_type":"markdown","source":"## References -\n - SKLearn CRF Documentation - https://sklearn-crfsuite.readthedocs.io/en/latest/index.html\n - ELI5 report documentation - https://media.readthedocs.org/pdf/eli5/0.4/eli5.pdf\n - POS tagging and NER - https://hpi.de/fileadmin/user_upload/fachgebiete/plattner/teaching/NaturalLanguageProcessing/NLP2015/NLP04_POS_NER.pdf\n - Survey of different NER techniques - https://nlp.cs.nyu.edu/sekine/papers/li07.pdf\n - NLTK book chapter 7 - http://www.nltk.org/book/ch07.html\n - Annotated GMB corpus dataset - https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/home\n - Understanding Model performance - https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}